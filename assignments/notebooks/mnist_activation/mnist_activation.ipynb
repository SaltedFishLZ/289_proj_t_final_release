{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import MNIST \n",
    "from torch.autograd import gradcheck\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Classification**\n",
    "In this part we will explore the performance of using different activation units to train on a subset of MNIST dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we load the dataset and turn it into numpy arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_train_set = MNIST(\"Data\", download = True, train = True)\n",
    "mnist_test_set = MNIST(\"Data\", download = True, train = False)\n",
    "mnist_trainX = np.array(mnist_train_set.data.numpy())\n",
    "mnist_trainY = np.array(mnist_train_set.targets.numpy())\n",
    "mnist_testX = np.array(mnist_test_set.data.numpy())\n",
    "mnist_testY = np.array(mnist_test_set.targets.numpy())\n",
    "mnist_trainX = mnist_trainX.reshape((mnist_trainX.shape[0], -1))\n",
    "mnist_testX = mnist_testX.reshape((mnist_testX.shape[0], -1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (a) Visualization\n",
    "\n",
    "Visualize a mnist data and its label. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################### TODO A\n",
    "\n",
    "############################################################### TODO A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell below, split the validation set with fraction 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b) Split training set and the validation set\n",
    "\n",
    "Split training set and the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "############################################# TODO B\n",
    "# Please Fill in the cell to implement X_train, Y_train, X_valid, Y_valid, X_test, and Y_test in numpy array\n",
    "# Also provide the number of data in n_train, n_valid, and n_test\n",
    "\n",
    "#############################################\n",
    "\n",
    "# Turn the numpy array to pytorch tensor\n",
    "\n",
    "X_train = torch.Tensor(X_train)\n",
    "Y_train = torch.from_numpy(Y_train)\n",
    "# split the validation set\n",
    "X_valid = torch.Tensor(X_valid)\n",
    "Y_valid = torch.from_numpy(Y_valid)\n",
    "# renaming the test data\n",
    "X_test = torch.Tensor(X_test)\n",
    "Y_test = torch.from_numpy(Y_test)\n",
    "\n",
    "print(\"Number of training data: {}\".format(n_train))\n",
    "print(\"Number of validation data: {}\".format(n_valid))\n",
    "print(\"Number of test data: {}\".format(n_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **(c) Implementing activation functions: Mish and Swish**\n",
    "Pytorch provides many activation units under torch.nn.Module.\n",
    "\n",
    "We can just use torch.nn.{Module Name} to instantiates the module. [Here is the reference of the official documentation on the types of activation functions and how to use them](https://pytorch.org/docs/stable/nn.html#non-linear-activations-weighted-sum-nonlinearity)\n",
    "\n",
    "Moreover, we can also define new activation units.\n",
    "\n",
    "In this problem, we will implement Mish and Swish activation function\n",
    "\n",
    "The original paper for Mish : https://arxiv.org/abs/1908.08681\n",
    "\n",
    "The original paper for Swish: https://arxiv.org/abs/1710.05941\n",
    "\n",
    "The Mish function is defined as:\n",
    "\n",
    "$f_{Mish}(x) = x(tanh(ln(1+e^x)))$\n",
    "\n",
    "And the Swish function is defined as: \n",
    "\n",
    "$f_{Swish}(x) = x(sigmoid(\\beta x)) = \\frac{x}{( 1 + e^{\\beta x})}$, where $\\beta$ is a learnable weight \n",
    "\n",
    "Implement the two function by completing forward and backward(gradient) method.\n",
    "You can use torch.gradcheck to see if your gradient is computed correctly. The torch.gradcheck compares the analytical gradients to numerical gradients, in which the analytical gradients are obtained by the backward methods and the numerical gradients are obtained by introducing small difference to the input.\n",
    "\n",
    "If your implementation is corrent. The gradcheck function will return True.\n",
    "For more implemenation details, you can see the official note from pytorch: https://pytorch.org/docs/stable/notes/extending.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is an example of how to implement an autograd function in pytorch\n",
    "# the forward function compute the output of the function \n",
    "# the backward function computes the gradient with respect to the input (and other parameters like weights)\n",
    "# For activation function, this can be easily done by chain rule.\n",
    "# As the example, the gradient w.r.t input equals the gradient w.r.t its output multiplied \n",
    "#    by the gradient of its output w.r.t its input \n",
    "# The ctx.saved_tensors can save the input and some tempory results from forward method\n",
    "# In the backward method, call ctx.saved_tensors to load the input and tempory results\n",
    "class SigmoidFunction(torch.autograd.Function):\n",
    "    \n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        output = 1.0/ ( 1 + torch.exp(-input))\n",
    "        ctx.save_for_backward(output)   # save for backward function\n",
    "        return output\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        output, = ctx.saved_tensors\n",
    "        grad_input = output * (1 - output) * grad_output\n",
    "        return grad_input\n",
    "\n",
    "class MishFunction(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        ################################################### TODO C_1\n",
    "\n",
    "        ###################################################\n",
    "        return output\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        ################################################### TODO C_2\n",
    "\n",
    "        ###################################################\n",
    "        return grad_input    \n",
    "    \n",
    "class SwishFunction(torch.autograd.Function):\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, input, beta):\n",
    "        ################################################### TODO C_3\n",
    "\n",
    "        ################################################### \n",
    "        return output\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        # The beta in swish function is learnable. Therefore you have to also compute the gradients w.r.t beta\n",
    "        ################################################### TODO C_4\n",
    "\n",
    "        ################################################### TODO\n",
    "        return grad_input, grad_beta\n",
    "    \n",
    "sigmoid = SigmoidFunction.apply\n",
    "mish = MishFunction.apply\n",
    "swish = SwishFunction.apply\n",
    "\n",
    "\n",
    "class Sigmoid(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(MySigmoid, self).__init__()\n",
    "\n",
    "    def forward(self, input):\n",
    "        return sigmoid(input)\n",
    "\n",
    "class Mish(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Mish, self).__init__()\n",
    "\n",
    "    def forward(self, input):\n",
    "        return sigmoid(input)\n",
    "    \n",
    "class Swish(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Swish, self).__init__()\n",
    "        self.beta = torch.nn.Parameter(torch.Tensor(1))\n",
    "        self.beta.data.uniform_(-0.1, 0.1)\n",
    "        \n",
    "    def forward(self, input):\n",
    "        return swish(input, self.beta)\n",
    "\n",
    "input = (torch.randn(50,50,dtype=torch.double,requires_grad=True))\n",
    "test_result = gradcheck(sigmoid, input, eps=1e-6, atol=1e-4)\n",
    "print(\"Gradient check for sigmoid function: \", test_result)\n",
    "input = (torch.randn(50,50,dtype=torch.double,requires_grad=True))\n",
    "test_result = gradcheck(mish, input, eps=1e-6, atol=1e-4)\n",
    "print(\"Gradient check for mish function: \", test_result)\n",
    "input = (torch.randn(50,50,dtype=torch.double,requires_grad=True), torch.randn(50,50,dtype=torch.double,requires_grad=True))\n",
    "test_result = gradcheck(swish, input, eps=1e-6, atol=1e-4)\n",
    "print(\"Gradient check for swish function: \", test_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (d) Training neural networks using different activation function\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D_in = mnist_trainX.shape[1]\n",
    "D_out = 10\n",
    "\n",
    "def training(activation_module, D_hidden_in, D_hidden_out, x, y, learning_rate, epochs):\n",
    "    \n",
    "    model = torch.nn.Sequential(\n",
    "        torch.nn.Linear(D_in, D_hidden_in),\n",
    "        activation_module(),\n",
    "        torch.nn.Linear(D_hidden_in, D_hidden_out),\n",
    "        activation_module(),\n",
    "        torch.nn.Linear(D_hidden_out, D_out),\n",
    "    )\n",
    "    \n",
    "    loss_train_his = []\n",
    "    loss_valid_his = []\n",
    "    acc_train_his = []\n",
    "    acc_valid_his = []\n",
    "\n",
    "    loss_fn = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "    for i in range(epochs):\n",
    "        \n",
    "        if i%10 == 9:\n",
    "            with torch.no_grad():\n",
    "                model.eval()\n",
    "                loss_valid, acc_valid = test(model, X_valid, Y_valid)\n",
    "                loss_valid_his.append(loss_valid)\n",
    "                acc_valid_his.append(acc_valid)\n",
    "                loss_train, acc_train = test(model, x, y)\n",
    "                loss_train_his.append(loss_train)\n",
    "                acc_train_his.append(acc_train)\n",
    "                print(i, \"loss = \", loss_train, \"acc = \", acc_train, \"valid: \", (loss_valid, acc_valid))\n",
    "        model.train()\n",
    "        y_pred = model(x)\n",
    "        loss = loss_fn(y_pred, y)\n",
    "            \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(\"Training complete\")\n",
    "    return model, loss_train_his, loss_valid_his, acc_train_his, acc_valid_his\n",
    "\n",
    "def test(model, X_test, Y_test):\n",
    "    \n",
    "    y_pred = model(X_test)\n",
    "    loss_fn = torch.nn.CrossEntropyLoss()\n",
    "    loss = loss_fn(y_pred, Y_test)\n",
    "    _, label = torch.max(y_pred, 1)\n",
    "    return loss.item(), (torch.count_nonzero(label == Y_test)/ Y_test.shape[0]).item()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the above training function, train the classifier using different activation function including ReLU, Sigmoid, LeakyReLU, and Tanh, Swish, and Mish.\n",
    "\n",
    "Record the returned losses and accuracy in the array losses and accs.\n",
    "\n",
    "First, using the same learning rate 0.005 for all activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1000\n",
    "learning_rate = 5e-3\n",
    "activation_units = [torch.nn.ReLU, torch.nn.Sigmoid, torch.nn.LeakyReLU, torch.nn.Tanh, Mish, Swish]\n",
    "activation_strs = [\"ReLU\", \"Sigmoid\", \"LeakyReLU\", \"Tanh\",  \"Mish\", \"Swish\"]\n",
    "learning_rate = [5e-3, 5e-3, 5e-3, 5e-3, 5e-3, 5e-3]\n",
    "models = []\n",
    "losses = np.zeros((len(activation_units), 2, int(epochs/10)))\n",
    "accs = np.zeros((len(activation_units), 2, int(epochs/10)))\n",
    "\n",
    "\n",
    "for i, act in enumerate(activation_units):\n",
    "    print(\"Training with {} activation units\".format(activation_strs[i]))\n",
    "# Call the training function to get the model, loss and accuracy\n",
    "    model, loss_train_his, loss_valid_his, acc_train_his, acc_valid_his = training(act, \n",
    "                200, 64, X_train, Y_train, learning_rate[i], epochs)\n",
    "    models.append(model)\n",
    "    losses[i,0,:] = np.array(loss_train_his)\n",
    "    losses[i,1,:] = np.array(loss_valid_his)\n",
    "    accs[i,0,:] = np.array(acc_train_his)\n",
    "    accs[i,1,:] = np.array(acc_valid_his)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the change of loss and accuracy versus epochs.\n",
    "\n",
    "You should have four plots (1) training loss (2) validation loss (3) training accuracy (4) validation accuracy\n",
    "\n",
    "Observe the loss and accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Your Observation:_\n",
    "\n",
    "We can see that the ReLU and LeakyReLU have similar loss during training, and their loss have converged. Their performance are the best compared with other activation functions.  For Tanh, and sigmoid activation units, the loss do decrease but have not converged, which indicate that we should slightly increase the learning rate or increase the number of epochs. For Mish activation function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_range = range(9, epochs, 10)\n",
    "for i, act in enumerate(activation_units):\n",
    "    plt.plot(x_range, losses[i,0], label = activation_strs[i])\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Training Losses\")\n",
    "plt.yscale(\"log\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "for i, act in enumerate(activation_units):\n",
    "    plt.plot(x_range, losses[i,1], label = activation_strs[i])\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Validation Losses\")\n",
    "plt.yscale(\"log\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "for i, act in enumerate(activation_units):\n",
    "    plt.plot(x_range, accs[i,0], label = activation_strs[i])\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Training Accuracy\")\n",
    "plt.yscale(\"log\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "for i, act in enumerate(activation_units):\n",
    "    plt.plot(x_range, accs[i,1], label = activation_strs[i])\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Validation Accuracy\")\n",
    "plt.yscale(\"log\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **(e) Grid search of learning rate for different activation functions**\n",
    "You should have noticed that some activation units did not converge.\n",
    "\n",
    "This means that the learning rate may be too low for those activation units.\n",
    "\n",
    "Adjust the learning rates for those activation units, and try to optimize the learning rate for each activation unit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_lr = 8\n",
    "epochs = 1000\n",
    "losses = np.zeros((len(activation_units), 2, num_lr, int(epochs/10)))\n",
    "accs = np.zeros((len(activation_units), 2, num_lr, int(epochs/10)))\n",
    "\n",
    "###################################################### TODO E\n",
    "\n",
    "######################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe which learning rate is best for each activation function.\n",
    "Then compare these activation function result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This may take about one hours to train all models if you use CPU training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_range = range(9, epochs, 10)\n",
    "for i, act in enumerate(activation_units):\n",
    "    for j, lr in enumerate(learning_rates[i]):\n",
    "        plt.plot(x_range, np.clip(losses[i,0,j], a_min = None, a_max = 100), label = \"lr = {:1.4f}\".format(lr))\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Training Losses\")\n",
    "    #plt.ylim(0, 5)\n",
    "    plt.yscale(\"log\")\n",
    "    plt.legend()\n",
    "    plt.title(activation_strs[i])\n",
    "    plt.show()\n",
    "\n",
    "for i, act in enumerate(activation_units):\n",
    "    for j, lr in enumerate(learning_rates[i]):\n",
    "        plt.plot(x_range, np.clip(losses[i,1,j], a_min = None, a_max = 100), label = \"lr = {:1.4f}\".format(lr))\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Validation Losses\")\n",
    "    #plt.ylim(0, 5)\n",
    "    plt.yscale(\"log\")\n",
    "    plt.legend()\n",
    "    plt.title(activation_strs[i])\n",
    "    plt.show()\n",
    "\n",
    "for i, act in enumerate(activation_units):\n",
    "    for j, lr in enumerate(learning_rates[i]):\n",
    "        plt.plot(x_range, accs[i,0,j], label = \"lr = {:1.4f}\".format(lr))\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Training Accuracy\")\n",
    "    #plt.ylim(0, 5)\n",
    "    plt.yscale(\"log\")\n",
    "    plt.legend()\n",
    "    plt.title(activation_strs[i])\n",
    "    plt.show()\n",
    "\n",
    "for i, act in enumerate(activation_units):\n",
    "    for j, lr in enumerate(learning_rates[i]):\n",
    "        plt.plot(x_range, accs[i,1,j], label = \"lr = {:1.4f}\".format(lr))\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Validation Accuracy\")\n",
    "    #plt.ylim(0, 5)\n",
    "    plt.yscale(\"log\")\n",
    "    plt.legend()\n",
    "    plt.title(activation_strs[i])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **(f) Training using the best learning rate**\n",
    "Using the best learning rate to train on the complete training set.\n",
    "\n",
    "Compare the loss and accuracy during training.\n",
    "\n",
    "Then report the test accuracy for each activation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_trainall = torch.Tensor(mnist_subsetX)\n",
    "Y_trainall = torch.from_numpy(mnist_subsetY)\n",
    "\n",
    "epochs = 1000\n",
    "\n",
    "models = []\n",
    "losses = np.zeros((len(activation_units), 2, int(epochs/10)))\n",
    "accs = np.zeros((len(activation_units), 2, int(epochs/10)))\n",
    "###################################################### TODO F\n",
    "\n",
    "######################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_range = range(9, epochs, 10)\n",
    "for i, act in enumerate(activation_units):\n",
    "    plt.plot(x_range, losses[i,0], label = activation_strs[i])\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Training Losses\")\n",
    "plt.yscale(\"log\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "for i, act in enumerate(activation_units):\n",
    "    plt.plot(x_range, accs[i,0], label = activation_strs[i])\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Training Accuracy\")\n",
    "#plt.yscale(\"log\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Report the accuracy on test set\n",
    "###################################################### TODO F\n",
    "\n",
    "###################################################### TODO"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
