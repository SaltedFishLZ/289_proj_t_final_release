{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import MNIST \n",
    "from torch.autograd import gradcheck\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Classification**\n",
    "In this part we will explore the performance of using different activation units to train on a subset of MNIST dataset\n",
    "\n",
    "Though we will using pytorch to train the model, \n",
    "You don't have to know the function definition and the class from pytorch.\n",
    "\n",
    "After doing this coding assignment, you will see how we can explore different activation function on training a fully-connected neural network. \n",
    "And see how we can visualize and compare different activation units by their learning rates and prediction accuracies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we load the dataset and turn it into numpy arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_train_set = MNIST(\"Data\", download = True, train = True)\n",
    "mnist_test_set = MNIST(\"Data\", download = True, train = False)\n",
    "mnist_trainX = np.array(mnist_train_set.data.numpy())\n",
    "mnist_trainY = np.array(mnist_train_set.targets.numpy())\n",
    "mnist_testX = np.array(mnist_test_set.data.numpy())\n",
    "mnist_testY = np.array(mnist_test_set.targets.numpy())\n",
    "mnist_trainX = mnist_trainX.reshape((mnist_trainX.shape[0], -1))\n",
    "mnist_testX = mnist_testX.reshape((mnist_testX.shape[0], -1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (a) Visualization\n",
    "\n",
    "Visualize a mnist data and its label. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################################################################################\n",
    "# TODO: randomly select 3 indices for training data and 3 indices for test data,\n",
    "#       and use matplotlib \"imshow\" function to plot the the data as figure.\n",
    "#       The training images are in the numpy array \"mnist_trainX\", the training labels are in the numpy array \"mnist_trainY\"\n",
    "#       The test     images are in the numpy array \"mnist_testX\" , the test     labels are in the numpy array \"mnist_testY\"\n",
    "# Hint: we have flattened the images, so you may have to convert them back to plot the images.\n",
    "#############################################################################################################\n",
    "\n",
    "############################################################### TODO A\n",
    "\n",
    "############################################################### TODO A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell below, split the validation set with fraction 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b) Split training set and the validation set\n",
    "\n",
    "Split training set and the validation set.\n",
    "\n",
    "First, you should get 20\\% of the \"mnist_trainX\" as our data, because we just want to use a subset of the original training set.\n",
    "Then, split 80\\% of the data to be our training data, and the remaining 20\\% as our validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#############################################################################################################\n",
    "# Please Fill in the cell to implement X_train, Y_train, X_valid, Y_valid in numpy array\n",
    "# Also provide the number of data in n_valid, and n_test\n",
    "# The raw data is loaded in the numpy arrays \"mnist_trainX\" and \"mnist_trainY\"\n",
    "#     Definition of the required variable:\n",
    "#          X_train: the training images\n",
    "#          Y_train: the training labels\n",
    "#          X_valid: the validation images\n",
    "#          Y_train: the validation labels\n",
    "#          n_train: the number of training data\n",
    "#          n_valid: the number of validation data\n",
    "#############################################################################################################\n",
    "\n",
    "# the split fraction\n",
    "train_frac = 0.2\n",
    "valid_frac = 0.2\n",
    "\n",
    "############################################# TODO B\n",
    "n_train = 0\n",
    "n_valid = 0\n",
    "\n",
    "X_train = np.array([])\n",
    "Y_train = np.array([])\n",
    "\n",
    "X_valid = np.array([])\n",
    "Y_valid = np.array([])\n",
    "#############################################\n",
    "# renaming the test data\n",
    "X_test = mnist_testX\n",
    "Y_test = mnist_testY\n",
    "n_test = mnist_testX.shape[0]\n",
    "# Turn the numpy array to pytorch tensor\n",
    "X_train = torch.Tensor(X_train)\n",
    "Y_train = torch.from_numpy(Y_train)\n",
    "\n",
    "X_valid = torch.Tensor(X_valid)\n",
    "Y_valid = torch.from_numpy(Y_valid)\n",
    "\n",
    "X_test = torch.Tensor(X_test)\n",
    "Y_test = torch.from_numpy(Y_test)\n",
    "\n",
    "print(\"Number of training data: {}\".format(n_train))\n",
    "print(\"Number of validation data: {}\".format(n_valid))\n",
    "print(\"Number of test data: {}\".format(n_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **(c) Implementing activation functions: Mish and Swish**\n",
    "Pytorch provides many activation units under torch.nn.Module.\n",
    "\n",
    "We can just use torch.nn.{Module Name} to instantiates the module. [Here is the reference of the official documentation on the types of activation functions and how to use them](https://pytorch.org/docs/stable/nn.html#non-linear-activations-weighted-sum-nonlinearity)\n",
    "\n",
    "Moreover, we can also define new activation units.\n",
    "\n",
    "In this problem, we will implement Mish and Swish activation function\n",
    "\n",
    "The original paper for Mish : https://arxiv.org/abs/1908.08681\n",
    "\n",
    "The original paper for Swish: https://arxiv.org/abs/1710.05941\n",
    "\n",
    "The Mish function is defined as:\n",
    "\n",
    "$f_{Mish}(x) = x(tanh(ln(1+e^x)))$\n",
    "\n",
    "And the Swish function is defined as: \n",
    "\n",
    "$f_{Swish}(x) = x(sigmoid(\\beta x)) = \\frac{x}{( 1 + e^{\\beta x})}$, where $\\beta$ is a learnable weight \n",
    "\n",
    "Implement the two function by completing forward and backward(gradient) method.\n",
    "You can use torch.gradcheck to see if your gradient is computed correctly. The torch.gradcheck compares the analytical gradients to numerical gradients, in which the analytical gradients are obtained by the backward methods and the numerical gradients are obtained by introducing small difference to the input.\n",
    "\n",
    "If your implementation is corrent. The gradcheck function will return True.\n",
    "For more implemenation details, you can see the official note from pytorch: https://pytorch.org/docs/stable/notes/extending.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is an example of how to implement an autograd function in pytorch\n",
    "# the forward function compute the output of the function \n",
    "# the backward function computes the gradient with respect to the input (and other parameters like weights)\n",
    "# For activation function, this can be easily done by chain rule.\n",
    "# As the example, the gradient w.r.t input equals the gradient w.r.t its output multiplied \n",
    "#    by the gradient of its output w.r.t its input \n",
    "# The ctx.saved_tensors can save the input and some tempory results from forward method\n",
    "# In the backward method, call ctx.saved_tensors to load the input and tempory results\n",
    "class SigmoidFunction(torch.autograd.Function):\n",
    "    \n",
    "    @staticmethod\n",
    "    def forward(ctx, x):\n",
    "        output = 1.0/ ( 1 + torch.exp(-x))\n",
    "        ctx.save_for_backward(output)   # save for backward function\n",
    "        return output\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        output, = ctx.saved_tensors\n",
    "        grad_input = output * (1 - output) * grad_output\n",
    "        return grad_input\n",
    "\n",
    "class MishFunction(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x):\n",
    "        ##########################################################################################\n",
    "        # TODO: implement the forward function for mish\n",
    "        #       You should compute the intermediate result of the tanh function and the output:\n",
    "        #       Input: \n",
    "        #           x: the input variable to the function\n",
    "        #       Variable to implement:\n",
    "        #           tanh_ : compute the tanh( ln (1+e^x) )\n",
    "        #           output: the output of the mish function, using the intermediate result tanh_\n",
    "        # Hint: using functions provided by pytorch: torch.tanh, torch.log, and torch.exp\n",
    "        #       We will use the intermediate result to reduce computation loads for the backward function\n",
    "        ##########################################################################################\n",
    "        ################################################### TODO C_1\n",
    "        tanh_ = 0\n",
    "        output = 0 \n",
    "        ###################################################\n",
    "        ctx.save_for_backward(x, tanh_)\n",
    "        return output\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        x, tanh_ = ctx.saved_tensors\n",
    "        ##########################################################################################\n",
    "        # TODO: implement the backward function for mish\n",
    "        #       The original input x and the intermediate result tanh_ is available.\n",
    "        #       You should compute gradient with w.r.t the input of the mish :\n",
    "        #       Input: \n",
    "        #           grad_output : the gradient of the loss function w.r.t the \"output\" of the mish function\n",
    "        #       Variable to implement:\n",
    "        #           grad_input  : the gradient of the loss function w.r.t the \"input\" of the mish :\n",
    "        # Hint: using functions provided by pytorch: torch.exp, and using the intermediate result tanh_\n",
    "        ##########################################################################################\n",
    "        ################################################### TODO C_2\n",
    "        grad_input = 0\n",
    "        ###################################################\n",
    "        return grad_input    \n",
    "    \n",
    "class SwishFunction(torch.autograd.Function):\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, x, beta):\n",
    "        ##########################################################################################\n",
    "        # TODO: implement the forward function for swish\n",
    "        #       You should compute the intermediate result of the swish function and the output:\n",
    "        #       Input: \n",
    "        #           x: the input variable to the function\n",
    "        #       beta:  the learnable parameter of the swish\n",
    "        #       Variable to implement:\n",
    "        #           beta_sigmoid : compute the sigmoid(beta*x)\n",
    "        #           output: the output of the swish function, using the intermediate result beta_sigmoid\n",
    "        # Hint: using functions provided by pytorch: torch.exp or torch.sigmoid\n",
    "        #       We will use the intermediate result to reduce computation loads for the backward function\n",
    "        ##########################################################################################\n",
    "        ################################################### TODO C_3\n",
    "        beta_sigmoid = 0\n",
    "        output = 0\n",
    "        ################################################### \n",
    "        ctx.save_for_backward(x, beta, beta_sigmoid)\n",
    "        return output\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        # The beta in swish function is learnable. Therefore you have to also compute the gradients w.r.t beta\n",
    "        x, beta, beta_sigmoid = ctx.saved_tensors\n",
    "        ##########################################################################################\n",
    "        # TODO: implement the backward function for swish\n",
    "        #       The original input x, learnable parameter beta and the intermediate result beta_sigmoid is available.\n",
    "        #       You should compute gradient with w.r.t the input and the learnable parameter of the mish :\n",
    "        #       Input: \n",
    "        #           grad_output : the gradient of the loss function w.r.t the \"output\" of the swish function\n",
    "        #       Variable to implement:\n",
    "        #           grad_input  : the gradient of the loss function w.r.t the \"input\" of the swish :\n",
    "        #           grad_beta   : the gradient of the loss function w.r.t the parameter beta:\n",
    "        ##########################################################################################\n",
    "        ################################################### TODO C_4\n",
    "        grad_input = 0\n",
    "        grad_beta  = 0\n",
    "        ################################################### TODO\n",
    "        return grad_input, grad_beta\n",
    "    \n",
    "sigmoid = SigmoidFunction.apply\n",
    "mish = MishFunction.apply\n",
    "swish = SwishFunction.apply\n",
    "\n",
    "\n",
    "class Sigmoid(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(MySigmoid, self).__init__()\n",
    "\n",
    "    def forward(self, input):\n",
    "        return sigmoid(input)\n",
    "\n",
    "class Mish(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Mish, self).__init__()\n",
    "\n",
    "    def forward(self, input):\n",
    "        return mish(input)\n",
    "    \n",
    "class Swish(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Swish, self).__init__()\n",
    "        self.beta = torch.nn.Parameter(torch.Tensor(1))\n",
    "        self.beta.data.uniform_(-0.1, 0.1)\n",
    "        \n",
    "    def forward(self, input):\n",
    "        return swish(input, self.beta)\n",
    "\n",
    "input = (torch.randn(50,50,dtype=torch.double,requires_grad=True))\n",
    "test_result = gradcheck(sigmoid, input, eps=1e-6, atol=1e-4)\n",
    "print(\"Gradient check for sigmoid function: \", test_result)\n",
    "input = (torch.randn(50,50,dtype=torch.double,requires_grad=True))\n",
    "test_result = gradcheck(mish, input, eps=1e-6, atol=1e-4)\n",
    "print(\"Gradient check for mish function: \", test_result)\n",
    "input = (torch.randn(50,50,dtype=torch.double,requires_grad=True), torch.randn(50,50,dtype=torch.double,requires_grad=True))\n",
    "test_result = gradcheck(swish, input, eps=1e-6, atol=1e-4)\n",
    "print(\"Gradient check for swish function: \", test_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (d) Training neural networks using different activation function\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D_in = mnist_trainX.shape[1]\n",
    "D_out = 10\n",
    "\n",
    "def training(activation_module, D_hidden_in, D_hidden_out, x, y, learning_rate, epochs):\n",
    "    \n",
    "    model = torch.nn.Sequential(\n",
    "        torch.nn.Linear(D_in, D_hidden_in),\n",
    "        activation_module(),\n",
    "        torch.nn.Linear(D_hidden_in, D_hidden_out),\n",
    "        activation_module(),\n",
    "        torch.nn.Linear(D_hidden_out, D_out),\n",
    "    )\n",
    "    \n",
    "    loss_train_his = []\n",
    "    loss_valid_his = []\n",
    "    acc_train_his = []\n",
    "    acc_valid_his = []\n",
    "\n",
    "    loss_fn = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "    for i in range(epochs):\n",
    "        \n",
    "        if i%10 == 9:\n",
    "            with torch.no_grad():\n",
    "                model.eval()\n",
    "                loss_valid, acc_valid = test(model, X_valid, Y_valid)\n",
    "                loss_valid_his.append(loss_valid)\n",
    "                acc_valid_his.append(acc_valid)\n",
    "                loss_train, acc_train = test(model, x, y)\n",
    "                loss_train_his.append(loss_train)\n",
    "                acc_train_his.append(acc_train)\n",
    "                print(i, \"loss = \", loss_train, \"acc = \", acc_train, \"valid: \", (loss_valid, acc_valid))\n",
    "        model.train()\n",
    "        y_pred = model(x)\n",
    "        loss = loss_fn(y_pred, y)\n",
    "            \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(\"Training complete\")\n",
    "    return model, loss_train_his, loss_valid_his, acc_train_his, acc_valid_his\n",
    "\n",
    "def test(model, X_test, Y_test):\n",
    "    \n",
    "    y_pred = model(X_test)\n",
    "    loss_fn = torch.nn.CrossEntropyLoss()\n",
    "    loss = loss_fn(y_pred, Y_test)\n",
    "    _, label = torch.max(y_pred, 1)\n",
    "    return loss.item(), (torch.count_nonzero(label == Y_test)/ Y_test.shape[0]).item()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the above training function, train the classifier using different activation function including ReLU, Sigmoid, LeakyReLU, and Tanh, Swish, and Mish.\n",
    "\n",
    "Record the returned losses and accuracy in the array losses and accs.\n",
    "\n",
    "First, using the same learning rate 0.005 for all activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1000\n",
    "learning_rate = 5e-3\n",
    "activation_units = [torch.nn.ReLU, torch.nn.Sigmoid, torch.nn.LeakyReLU, torch.nn.Tanh, Mish, Swish]\n",
    "activation_strs = [\"ReLU\", \"Sigmoid\", \"LeakyReLU\", \"Tanh\",  \"Mish\", \"Swish\"]\n",
    "learning_rate = [5e-3, 5e-3, 5e-3, 5e-3, 5e-3, 5e-3]\n",
    "models = []\n",
    "losses = np.zeros((len(activation_units), 2, int(epochs/10)))\n",
    "accs = np.zeros((len(activation_units), 2, int(epochs/10)))\n",
    "\n",
    "\n",
    "for i, act in enumerate(activation_units):\n",
    "    print(\"Training with {} activation units\".format(activation_strs[i]))\n",
    "# Call the training function to get the model, loss and accuracy\n",
    "    model, loss_train_his, loss_valid_his, acc_train_his, acc_valid_his = training(act, \n",
    "                200, 64, X_train, Y_train, learning_rate[i], epochs)\n",
    "    models.append(model)\n",
    "    losses[i,0,:] = np.array(loss_train_his)\n",
    "    losses[i,1,:] = np.array(loss_valid_his)\n",
    "    accs[i,0,:] = np.array(acc_train_his)\n",
    "    accs[i,1,:] = np.array(acc_valid_his)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the change of loss and accuracy versus epochs.\n",
    "\n",
    "You should have four plots (1) training loss (2) validation loss (3) training accuracy (4) validation accuracy\n",
    "\n",
    "Observe the loss and accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Your Observation:_**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_range = range(9, epochs, 10)\n",
    "for i, act in enumerate(activation_units):\n",
    "    plt.plot(x_range, losses[i,0], label = activation_strs[i])\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Training Losses\")\n",
    "plt.yscale(\"log\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "for i, act in enumerate(activation_units):\n",
    "    plt.plot(x_range, losses[i,1], label = activation_strs[i])\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Validation Losses\")\n",
    "plt.yscale(\"log\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "for i, act in enumerate(activation_units):\n",
    "    plt.plot(x_range, accs[i,0], label = activation_strs[i])\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Training Accuracy\")\n",
    "plt.yscale(\"log\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "for i, act in enumerate(activation_units):\n",
    "    plt.plot(x_range, accs[i,1], label = activation_strs[i])\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Validation Accuracy\")\n",
    "plt.yscale(\"log\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **(e) Grid search of learning rate for different activation functions**\n",
    "You should have noticed that some activation units did not converge.\n",
    "\n",
    "This means that the learning rate may be too low for those activation units.\n",
    "\n",
    "Adjust the learning rates for those activation units, and try to optimize the learning rate for each activation unit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_lr = 8\n",
    "epochs = 1000\n",
    "losses = np.zeros((len(activation_units), 2, num_lr, int(epochs/10)))\n",
    "accs = np.zeros((len(activation_units), 2, num_lr, int(epochs/10)))\n",
    "#############################################################################################################\n",
    "# TODO: set the learning_rates array so that we can search through different learning rates for each activation functions\n",
    "# Example: by setting learning_rates = [0.1, 0.2]*len(activation_units), the below code will train\n",
    "#          using learning rates 0.1 and 0.2 for each activation function neural network. \n",
    "#############################################################################################################\n",
    "\n",
    "###################################################### TODO E\n",
    "learning_rates = []*len(activation_units)\n",
    "######################################################\n",
    "for i, act in enumerate(activation_units):\n",
    "    for j, lr in enumerate(learning_rates[i]):\n",
    "        print(\"Training with {} activation units at learning rate {}\".format(activation_strs[i], lr))\n",
    "\n",
    "    # Call the training function to get the model, loss and accuracy\n",
    "        model, loss_train_his, loss_valid_his, acc_train_his, acc_valid_his = training(act, \n",
    "                    200, 64, X_train, Y_train, lr, epochs)\n",
    "        models.append(model)\n",
    "        losses[i,0,j,:] = np.array(loss_train_his)\n",
    "        losses[i,1,j,:] = np.array(loss_valid_his)\n",
    "        accs[i,0,j,:] = np.array(acc_train_his)\n",
    "        accs[i,1,j,:] = np.array(acc_valid_his)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe which learning rate is best for each activation function.\n",
    "Then compare these activation function result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This may take about one hours to train all models if you use CPU training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_range = range(9, epochs, 10)\n",
    "for i, act in enumerate(activation_units):\n",
    "    for j, lr in enumerate(learning_rates[i]):\n",
    "        plt.plot(x_range, np.clip(losses[i,0,j], a_min = None, a_max = 100), label = \"lr = {:1.4f}\".format(lr))\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Training Losses\")\n",
    "    #plt.ylim(0, 5)\n",
    "    plt.yscale(\"log\")\n",
    "    plt.legend()\n",
    "    plt.title(activation_strs[i])\n",
    "    plt.show()\n",
    "\n",
    "for i, act in enumerate(activation_units):\n",
    "    for j, lr in enumerate(learning_rates[i]):\n",
    "        plt.plot(x_range, np.clip(losses[i,1,j], a_min = None, a_max = 100), label = \"lr = {:1.4f}\".format(lr))\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Validation Losses\")\n",
    "    #plt.ylim(0, 5)\n",
    "    plt.yscale(\"log\")\n",
    "    plt.legend()\n",
    "    plt.title(activation_strs[i])\n",
    "    plt.show()\n",
    "\n",
    "for i, act in enumerate(activation_units):\n",
    "    for j, lr in enumerate(learning_rates[i]):\n",
    "        plt.plot(x_range, accs[i,0,j], label = \"lr = {:1.4f}\".format(lr))\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Training Accuracy\")\n",
    "    #plt.ylim(0, 5)\n",
    "    plt.yscale(\"log\")\n",
    "    plt.legend()\n",
    "    plt.title(activation_strs[i])\n",
    "    plt.show()\n",
    "\n",
    "for i, act in enumerate(activation_units):\n",
    "    for j, lr in enumerate(learning_rates[i]):\n",
    "        plt.plot(x_range, accs[i,1,j], label = \"lr = {:1.4f}\".format(lr))\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Validation Accuracy\")\n",
    "    #plt.ylim(0, 5)\n",
    "    plt.yscale(\"log\")\n",
    "    plt.legend()\n",
    "    plt.title(activation_strs[i])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **(f) Training using the best learning rate**\n",
    "Using the best learning rate to train on the complete training set.\n",
    "\n",
    "Compare the loss and accuracy during training.\n",
    "\n",
    "Then report the test accuracy for each activation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_trainall = torch.Tensor(mnist_subsetX)\n",
    "Y_trainall = torch.from_numpy(mnist_subsetY)\n",
    "\n",
    "epochs = 1000\n",
    "\n",
    "models = []\n",
    "losses = np.zeros((len(activation_units), 2, int(epochs/10)))\n",
    "accs = np.zeros((len(activation_units), 2, int(epochs/10)))\n",
    "#############################################################################################################\n",
    "# TODO: set the below variable \"learning rate\" to a list that contains the best learning rate for each activation function.\n",
    "#############################################################################################################\n",
    "\n",
    "###################################################### TODO F\n",
    "learning_rate = [0, 0, 0, 0, 0, 0]\n",
    "######################################################\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, act in enumerate(activation_units):\n",
    "    print(\"Training with {} activation units\".format(activation_strs[i]))\n",
    "    model, loss_train_his, loss_valid_his, acc_train_his, acc_valid_his = training(act, \n",
    "                200, 64, X_trainall, Y_trainall, learning_rate[i], epochs)\n",
    "    models.append(model)\n",
    "    losses[i,0,:] = np.array(loss_train_his)\n",
    "    losses[i,1,:] = np.array(loss_valid_his)\n",
    "    accs[i,0,:] = np.array(acc_train_his)\n",
    "    accs[i,1,:] = np.array(acc_valid_his)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_range = range(9, epochs, 10)\n",
    "for i, act in enumerate(activation_units):\n",
    "    plt.plot(x_range, losses[i,0], label = activation_strs[i])\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Training Losses\")\n",
    "plt.yscale(\"log\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "for i, act in enumerate(activation_units):\n",
    "    plt.plot(x_range, accs[i,0], label = activation_strs[i])\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Training Accuracy\")\n",
    "#plt.yscale(\"log\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################################################################################\n",
    "# TODO: Report the accuracy on test set, using the test function defined in part (d) to \n",
    "#       get the test accuracy of the model.\n",
    "#       The function test returns the loss and the accuracy\n",
    "#       Save the accuracy in the variable \"acc\"\n",
    "#############################################################################################################\n",
    "for i, model in enumerate(models):\n",
    "    \n",
    "    ###################################################### TODO\n",
    "    acc = 0\n",
    "    ###################################################### TODO\n",
    "    print(\"Accuracy of using {} activation units at learning rate {:1.4f}: {:1.4f}\".format(activation_strs[i].rjust(9), learning_rate[i], acc))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **(g) Visualize the learned swish activation unit**\n",
    "Get the learned parameters of the swish activation unit.\n",
    "Visualize the activation unit.\n",
    "\n",
    "Fill the missing code to use matplotlib to plot the learned swish activation.\n",
    "We have computed the function input x and the output y (stored in \"swish_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta1 = list(models[5].parameters())[2].data\n",
    "beta2 = list(models[5].parameters())[5].data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-100, 100, 100)\n",
    "\n",
    "swish_data = swish(torch.Tensor(x), beta1).detach().numpy()\n",
    "#############################################################################################################\n",
    "# TODO: Fill the missing code to plot the swish activation function using \"x\" and \"swish_data\"\n",
    "#############################################################################################################\n",
    "##################################################TODO G\n",
    "# plot the first swish function learned\n",
    "\n",
    "\n",
    "\n",
    "##################################################\n",
    "\n",
    "swish_data = swish(torch.Tensor(x), beta2).detach().numpy()\n",
    "\n",
    "##################################################TODO G\n",
    "# plot the second swish function learned\n",
    "\n",
    "\n",
    "\n",
    "##################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
