{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Initialization and Activation Units: A Numerical Stability's Perspective\n",
    "\n",
    "This assignment will use the following external packages: pytorch == 1.6.0\n",
    "\n",
    "\n",
    "*** Although we use PyTorch intensively in this coding problem, students don't need to touch PyTorch codes.***\n",
    "\n",
    "*** They only needs to deal with NumPy codes ***"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "source": [
    "## (a) Floating-point representations of real numbers\n",
    "\n",
    "Mathematically, our models are typically represented by real numbers with infinite precision. However, when programming, it is inevtiable that we can only use representations with limited precision. For example, our data are usually stored as floating-point numbers. This short question requires you to recall such basic concepts in computer science. Please read the following codes and answer the followinng questions in your comments.\n",
    "\n",
    "1. Why ``print(a)`` shows ``0.0``? Why ``print(b)`` shows ``inf``?\n",
    "\n",
    "2. Why ``print(c)`` and ``print(d)`` can give you the correct values, but ``print(c + d - d)``, ``print(c * c)`` or ``print(d * d)`` cannot?\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "a = 1e-10000\n",
    "print(a)\n",
    "\n",
    "b = 1e+10000\n",
    "print(b)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = 1e-250\n",
    "print(c)\n",
    "\n",
    "d = 1e+250\n",
    "print(d)\n",
    "\n",
    "print(c + d - d)\n",
    "print(c * c)\n",
    "print(d * d)"
   ]
  },
  {
   "source": [
    "### start (a)\n",
    "\n",
    "\n",
    "### end (a)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## (c) Gradient vanishing and exploding - Theory 2\n",
    "\n",
    "This part aims to give students some intuitions on on numerical issues we talked about in previous parts. Also, it helps you verify you answers in previous parts because you can try different experiments to examine the formula you get.\n",
    "\n",
    "\n",
    "Without loss of generality, we use **normal distribution with zero mean for all random varibales**. Also, for all internal layers ($l \\neq 0, l \\neq L$), we assume that $ n^{(l-1)} =  n^{(l)} = N$, $ F_{ \\mathbf{W}^{(l-1)} } = F_{ \\mathbf{W}^{(l)} }$ (their weights have the same distribution), and  $ F_{ (\\sigma')^{(l-1)} } = F_{ (\\sigma')^{(l)} }$ (those diagnoal matrices have the same distribution). In the code, the last assumption is acchieved by make $ z_{i}^{(l)}$ has the same distribution. For boundary conditions, we assume $n^{(0)} = n^{(L)} = 1$, you can imagine that we are trying to fit a $1$-D function.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "The following function is designed to sample a concrete $ \\nabla_{ \\mathbf{W}^{(l)} } L $ using the normal distribution assumption. The input arguments are `gamma_s_prime` ($\\gamma_{\\sigma'}$), `gamma_w` ($ \\gamma_{W} $), `gamma_nabla_y` ($ \\gamma_{\\nabla_{y} L} $), and `gamma_a` ($ \\gamma_{a} $),.\n",
    "It means that,\n",
    "\n",
    "$$ \\sigma' ( z_{i}^{(l)} ) \\sim \\mathcal{N} (0, \\gamma_{\\sigma'}^2 ) $$\n",
    "\n",
    "$$ W_{ij}^{(l)} \\sim \\mathcal{N} (0, \\gamma_{W}^2 ) $$\n",
    "\n",
    "$$ \\nabla_{y} L [i] \\sim \\mathcal{N} (0, \\gamma_{\\nabla_{y} L}^2 ) $$\n",
    "\n",
    "$$ a^{(l)}_{i} \\sim \\mathcal{N} (0, \\gamma_{a}^2 ) $$\n",
    "\n",
    "For  meanings of these symbols, please review part (b). Then, **please fill the codes in the `grad_wrt_weight` function**. You may need to review part(b)'s formulas."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the gradient of loss w.r.t. the the l-th layer's weight\n",
    "def grad_wrt_weight(gamma_s_prime, gamma_w, gamma_nabla_y, gamma_a, L, l, N, activation=\"tanh\"):\n",
    "    assert (gamma_nabla_y > 0), ValueError\n",
    "    assert (gamma_s_prime > 0), ValueError\n",
    "    assert (gamma_w > 0), ValueError\n",
    "    assert (L > 2), ValueError\n",
    "    assert (l > 1), ValueError(\"Don't use the firt layer!\")\n",
    "    assert (l < L), ValueError(\"Don't use the last layer!\")\n",
    "    assert (N > 1), ValueError\n",
    "\n",
    "    # for the last layer\n",
    "    delta = gamma_nabla_y * np.random.randn(N)\n",
    "\n",
    "    for i in range(L, l, -1):\n",
    "        # sample a new couple of matrices for each iteration\n",
    "        diag_s_prime = None\n",
    "        W = None\n",
    "\n",
    "        ### TODO:\n",
    "        ### fill codes using the backpropagation formula\n",
    "        ### you need to calculate diag_s_prime and W\n",
    "        ### start c-1 ###\n",
    "\n",
    "        ### end c-1 ###\n",
    "\n",
    "        delta =  W.T @ diag_s_prime @ delta\n",
    "\n",
    "    s_prime = gamma_s_prime * np.random.randn(N)\n",
    "    diag_s_prime = np.diag(s_prime)\n",
    "    delta = diag_s_prime @ delta\n",
    "\n",
    "    a = gamma_a * np.random.randn(N)\n",
    "    grad = np.outer(delta, a.T)\n",
    "\n",
    "    return(grad)"
   ]
  },
  {
   "source": [
    "\n",
    "\n",
    "Now, the following codes will generate such gradient matrices and calculate the emperical variance of all the elements.\n",
    "\n",
    "Here, we split the parameters and the estimation part to avoid run the estimation each time you change parameters."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### try to change parameters here \n",
    "L = 8\n",
    "l = 2\n",
    "N = 64\n",
    "gamma_s_prime = 2.0\n",
    "gamma_w = 2.0\n",
    "gamma_nabla_y = 2.0\n",
    "gamma_a = 2.0\n",
    "###\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repreat_n = 1000\n",
    "\n",
    "# sample repreat_n times\n",
    "nabla_w_list = []\n",
    "for _ in range(repreat_n):\n",
    "    nabla_w = grad_wrt_weight(gamma_s_prime=gamma_s_prime,\n",
    "                              gamma_w=gamma_w,\n",
    "                              gamma_nabla_y=gamma_nabla_y,\n",
    "                              gamma_a=gamma_a,\n",
    "                              L=L, l=l, N=N)\n",
    "    nabla_w_list.append(nabla_w)\n",
    "\n",
    "nabla_w_list = np.array(nabla_w_list)\n",
    "\n",
    "flattened_nabla_w = nabla_w_list.reshape(-1)\n",
    "empirical_variance = np.var(flattened_nabla_w)\n",
    "\n",
    "\n",
    "print(empirical_variance)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# visualization\n",
    "magnitude = np.log(np.abs(flattened_nabla_w))\n",
    "plt.hist(magnitude, bins=100, density=True)\n",
    "plt.xlabel(\"Magnitude (base 10)\")\n",
    "plt.ylabel(\"Density\")\n",
    "plt.show()"
   ]
  },
  {
   "source": [
    "The function below is the theoretical variance derived from part (b). Please **fill the codes**."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_theoretical_variance(gamma_s_prime, gamma_w, gamma_nabla_y, gamma_a, L, l, N):\n",
    "    assert (gamma_nabla_y > 0), ValueError\n",
    "    assert (gamma_s_prime > 0), ValueError\n",
    "    assert (gamma_w > 0), ValueError\n",
    "    assert (L > 2), ValueError\n",
    "    assert (l > 1), ValueError(\"Don't use the firt layer!\")\n",
    "    assert (l < L), ValueError(\"Don't use the last layer!\")\n",
    "    assert (N > 1), ValueError\n",
    "\n",
    "    variance = None\n",
    "    ### TODO:\n",
    "    ### fill codes here according to your results in the last part\n",
    "    ### start c-2 ###\n",
    "\n",
    "    ### end c-2 ###\n",
    "    return variance"
   ]
  },
  {
   "source": [
    "You should see that your theoretical variance has the same magnitude with the empirical variance estimation by running the codes below."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theoretical_variance = get_theoretical_variance(gamma_s_prime=gamma_s_prime,\n",
    "                                                gamma_w=gamma_w,\n",
    "                                                gamma_nabla_y=gamma_nabla_y,\n",
    "                                                gamma_a=gamma_a,\n",
    "                                                L=L, l=l, N=N)\n",
    "print(theoretical_variance)\n",
    "\n",
    "\n"
   ]
  },
  {
   "source": [
    "Now please change the parameters and answer the following questions.\n",
    "\n",
    "(1) Increase the number of layers ($L$) to 64, increase $N$ to 256, increase `gamma_w` to 1024.0 and re-rerun the above program. What will happen? Why?\n",
    "\n",
    "(2) Keep the parameters the same as (1), change `gamma_w` to 0.001 and and re-rerun the above program. What will happen? Why?"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### start (c)-3\n",
    "<font color='blue'>\n",
    "\n",
    "</font>\n",
    "### end (c)-3"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## (d) Graident vanishing and exploding - Practice in PyTorch\n",
    "\n",
    "From (a), we can have a rough sense that if our deep neural networks stack many layers with extremely small/large values, it is very possible to encounter numerical issues similar to (a) when using back-propagation to optimize our model. In this part, we will use concrete examples to show you that what will happen in real neural networks with non-linear activation units if we don't initialize the parameters appropriately.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "How can we detect gradient vanishing? Here, we provide a helper function that can visualize the average of absolute values of gradients for each layer in the neural network. A simple rule is to check the magnitutes of gradients in the backward propagation path. If magnitudes of gradients decrease/increase drastically from the last layer to the first layer, it is very possible that we are encountered with a gradient vanishing/exploding."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_grad_flow(named_parameters, label=\"\"):\n",
    "    ave_grads = []\n",
    "    layers = []\n",
    "    for n, p in named_parameters:\n",
    "        if(p.requires_grad) and (\"bias\" not in n):\n",
    "            layers.append(n)\n",
    "            ave_grads.append(p.grad.abs().mean())\n",
    "    plt.plot(ave_grads, alpha=0.3, color=\"b\")\n",
    "    plt.hlines(0, 0, len(ave_grads)+1, linewidth=1, colors=\"b\" )\n",
    "    plt.xticks(range(0,len(ave_grads), 1), layers, rotation=\"vertical\")\n",
    "    plt.xlim(xmin=0, xmax=len(ave_grads))\n",
    "    plt.xlabel(\"Layers\")\n",
    "    plt.ylabel(\"Average Gradient Absolute Value\")\n",
    "    plt.title(\"Gradient flow\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n"
   ]
  },
  {
   "source": [
    "Now, please build a simple MLP consists of $L$ layers ($L \\gt 2$) as described in this problem. For each internal layer ($1 \\lt l \\lt L$), $n^{(l)} = n^{l+1} = N$. `n_0` and `n_L` in the codes corresponds to $n^{(0)}$ and $n^{(L)}$. You may use `nn.Sequential` to build a layer with non-linear activation units.\n",
    "\n",
    "Note:\n",
    "We don't use bias terms here."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_mlp(L, N, n_0=2, n_L=1, activation=\"sigmoid\"):\n",
    "    assert (L > 2), ValueError\n",
    "\n",
    "    input_layer = nn.Linear(n_0, N, bias=False)\n",
    "    output_layer = nn.Linear(N, n_L, bias=False)\n",
    "\n",
    "    layer_lists = []\n",
    "    for i in range(L-2):\n",
    "        # you should modify the internal_layer\n",
    "        internal_layer = None\n",
    "        if (activation == \"tanh\"):\n",
    "            internal_layer = nn.Sequential(\n",
    "                nn.Linear(N, N, bias=False),\n",
    "                nn.Tanh()\n",
    "            )\n",
    "        elif (activation == \"sigmoid\"):\n",
    "            internal_layer = nn.Sequential(\n",
    "                nn.Linear(N, N, bias=False),\n",
    "                nn.Sigmoid()\n",
    "            )\n",
    "        elif (activation == \"relu\"):\n",
    "            internal_layer = nn.Sequential(\n",
    "                nn.Linear(N, N, bias=False),\n",
    "                nn.ReLU()\n",
    "            )\n",
    "        else:\n",
    "            assert NotImplementedError\n",
    "\n",
    "        layer_lists.append(internal_layer)\n",
    "\n",
    "    layer_lists = [input_layer, ] + layer_lists + [output_layer, ]\n",
    "\n",
    "    return(nn.Sequential(*layer_lists))\n"
   ]
  },
  {
   "source": [
    "Although PyTorch often does a quite good initialization automatically when creating the model, in this part we require you to **implement different initialization using  NumPy** and **you cannot use other packages**. The following function is a wrapper. You need to implement `init_const`, `init_normal`, `init_xavier_normal`, `init_xavier_uniform`, `init_kaiming_normal` and 'init_kaiming_uniform' in the part and following parts."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(m, mean=0.0, var=1.0, method=\"default\"):\n",
    "    if type(m) == nn.Linear:\n",
    "        if (method == \"default\"):\n",
    "            pass\n",
    "        else:\n",
    "            # this wrapper will handle the conversion\n",
    "            # between PyTorch and NumPy\n",
    "            m.weight.data.fill_(0)\n",
    "            w = m.weight.detach().data.numpy()\n",
    "\n",
    "            if (method == \"const\"):\n",
    "                w = init_const(w, value=mean)\n",
    "            elif (method == \"normal\"):\n",
    "                w = init_normal(w, mean=mean, var=var)\n",
    "            elif (method == \"xavier_normal\"):\n",
    "                w = init_xavier_normal(w)\n",
    "            elif (method == \"xavier_uniform\"):\n",
    "                w = init_xavier_uniform(w)\n",
    "            elif (method == \"kaiming_normal\"):\n",
    "                w = init_kaiming_normal(w)\n",
    "            elif (method == \"kaiming_uniform\"):\n",
    "                w = init_kaiming_uniform(w)\n",
    "            else:\n",
    "                raise NotImplementedError\n",
    "\n",
    "            with torch.no_grad():\n",
    "                m.weight.data = torch.Tensor(w)\n",
    "    else:\n",
    "        # don't touch other layers\n",
    "        pass"
   ]
  },
  {
   "source": [
    "For this part, please implement `init_const` and `init_normal` in the following code block. `init_const` is constant initialization and fills the matrix with `value`. `init_normal` is normal distribution initialization and fills the matrix with random variables drawn from $\\mathcal{N}(\\mu, var)$."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_const(w, value):\n",
    "    assert (type(w) == np.ndarray), TypeError\n",
    "    assert len(w.shape) == 2, ValueError(\"W should be a matrix\")\n",
    "    result = w\n",
    "    ### TODO\n",
    "    ### start d-1\n",
    "\n",
    "    ### end d-1\n",
    "    return result\n",
    "\n",
    "\n",
    "def init_normal(w, mean, var):\n",
    "    assert (type(w) == np.ndarray), TypeError\n",
    "    assert len(w.shape) == 2, ValueError(\"W should be a matrix\")\n",
    "    result = w\n",
    "    ### TODO\n",
    "    ### start d-2\n",
    "\n",
    "    ### end d-2\n",
    "    return result\n"
   ]
  },
  {
   "source": [
    "You can check your implementations by printing out the weights like the following codes."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# demo\n",
    "model = create_mlp(L=4, N=2, n_0=2, n_L=1, activation=\"sigmoid\")\n",
    "\n",
    "print(\"=\" * 64)\n",
    "print(\"before\")\n",
    "print(\"=\" * 64)\n",
    "print(model[0].weight)\n",
    "\n",
    "for m in model.modules():\n",
    "    init_weights(m, mean=1.0, method=\"const\")\n",
    "\n",
    "print(\"=\" * 64)\n",
    "print(\"after\")\n",
    "print(\"=\" * 64)\n",
    "print(model[0].weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# demo\n",
    "model = create_mlp(L=4, N=2, n_0=2, n_L=1, activation=\"sigmoid\")\n",
    "\n",
    "print(model[0].weight)\n",
    "\n",
    "for m in model.modules():\n",
    "    init_weights(m, mean=1.0, var=1.0, method=\"normal\")\n",
    "\n",
    "print(model[0].weight)"
   ]
  },
  {
   "source": [
    "Now, let us get started with training a real neural network!!!"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def test(model, X, y):\n",
    "    model.eval()\n",
    "    y_pred = model(X)\n",
    "    loss_fn = torch.nn.MSELoss()\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    return(loss.item())\n",
    "\n",
    "def training(model, X, y, lr, optimizer=\"adam\", epochs=100, log_interval=100, visual_debug=False):\n",
    "    loss_train_his = []\n",
    "    loss_fn = torch.nn.MSELoss()\n",
    "\n",
    "    if (optimizer == \"adam\"):\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    elif (optimizer == \"sgd\"):\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    for i in range(epochs):\n",
    "\n",
    "\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        y_pred = model(X)\n",
    "        loss = loss_fn(y_pred, y)\n",
    "\n",
    "        loss_train = loss.item()\n",
    "        loss_train_his.append(loss_train)\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "\n",
    "        if (i % log_interval == log_interval - 1):\n",
    "            if visual_debug:\n",
    "                plot_grad_flow(model.named_parameters(), label=str(i))\n",
    "            print(\"epoch = {0:04d},  loss = \".format(i), loss_train)\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "    print(\"training complete\")\n",
    "    return model, loss_train_his"
   ]
  },
  {
   "source": [],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generating training daya\n",
    "X_train = torch.arange(-1, 1, 0.1)\n",
    "X_train_aug = torch.stack([X_train, torch.ones_like(X_train)], dim=1)\n",
    "y_train = torch.sin(10 * X_train_aug[:, :1])\n"
   ]
  },
  {
   "source": [
    "The following code cells are differnet initialization configurations. ** Run only one cell for each experiment and skip others.**"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # config 1\n",
    "# model = create_mlp(L=4, N=16, n_0=2, n_L=1, activation=\"sigmoid\")\n",
    "# for m in model.modules():\n",
    "#     init_weights(m, mean=0.0, method=\"const\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # config 2\n",
    "# model = create_mlp(L=4, N=16, n_0=2, n_L=1, activation=\"sigmoid\")\n",
    "# for m in model.modules():\n",
    "#     init_weights(m, mean=0.0, var=0.05, method=\"normal\")"
   ]
  },
  {
   "source": [
    "**Note: We changed the number of neurons in config 3**"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config 3\n",
    "model = create_mlp(L=4, N=128, n_0=2, n_L=1, activation=\"sigmoid\")\n",
    "for m in model.modules():\n",
    "    init_weights(m, mean=0.0, var=0.05, method=\"normal\")"
   ]
  },
  {
   "source": [
    "**Note: We changed the number of layers in config 4 !!!**"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # config 4\n",
    "# # We changed the layer numbers here !!!\n",
    "# model = create_mlp(L=32, N=128, n_0=2, n_L=1, activation=\"sigmoid\")\n",
    "# for m in model.modules():\n",
    "#     init_weights(m, mean=0.0, var=0.05, method=\"normal\")"
   ]
  },
  {
   "source": [
    "** Go back to config 3, now we only change the initial weights's variance. **"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # config 5\n",
    "# model = create_mlp(L=4, N=128, n_0=2, n_L=1, activation=\"sigmoid\")\n",
    "# for m in model.modules():\n",
    "#     init_weights(m, mean=0.0, var=1024.0, method=\"normal\")"
   ]
  },
  {
   "source": [
    "The following codes will train the neural network, visualize the training loss and plot the fitted function."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "epochs = 1000\n",
    "torch.manual_seed(0)\n",
    "\n",
    "model, loss_train_his = training(model, X_train_aug, y_train, lr=0.001, epochs=epochs, visual_debug=True)\n",
    "\n",
    "\n",
    "model.eval()\n",
    "y_pred = model(X_train_aug)\n",
    "\n",
    "X_train_np = X_train.detach().numpy()\n",
    "y_train_np = y_train.detach().numpy().reshape(-1)\n",
    "y_pred_np = y_pred.detach().numpy()\n",
    "# print(y_pred.shape)\n",
    "\n",
    "plt.scatter(X_train_np, y_train_np, label=\"true\", c=\"orange\")\n",
    "plt.plot(X_train_np, y_pred_np, label=\"fitted\")\n",
    "\n",
    "plt.title(\"Fitted Function\")\n",
    "plt.xlabel(\"x (input)\")\n",
    "plt.ylabel(\"y (output)\")\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "source": [
    "Now, please **try different initialization configurations (config 1 - 5 )** in the code cells above and **compare the results** of each configuration. Try to **analyze the results with the gradient flow plot**."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### start d\n",
    "\n",
    "\n",
    "### end d\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## (e) Xavier initialization - theory\n",
    "\n",
    "Please check the assignment document and derive the formula of Xavier initialization. We will use it in the next part."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## (f) Xavier initialization - practice\n",
    "\n",
    "Please **implement Xavier initialization** in the following codes. **You can only use NumPy**, although our codes will convert your results into PyTorch tensors later."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Xavier for normal distribution\n",
    "def init_xavier_normal(w):\n",
    "    assert (type(w) == np.ndarray), TypeError\n",
    "    assert len(w.shape) == 2, ValueError(\"W should be a matrix\")\n",
    "    result = w\n",
    "    ### TODO: your codes here\n",
    "    ### start f-1\n",
    "\n",
    "    ### end f-1\n",
    "    return(result)\n",
    "\n",
    "#  Xavier for normal distribution\n",
    "def init_xavier_uniform(w):\n",
    "    assert (type(w) == np.ndarray), TypeError\n",
    "    assert len(w.shape) == 2, ValueError(\"W should be a matrix\")\n",
    "    result = w\n",
    "    ### TODO: your codes here\n",
    "    ### start f-2\n",
    "\n",
    "    ### end f-2\n",
    "    return(result)\n"
   ]
  },
  {
   "source": [
    "## (h) Kaiming initialization - theory\n",
    "\n",
    "Please check the assignment document and derive the formula of Kaiming initialization. We will use it in the next part."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## (i) Kaiming initialization - practice\n",
    "\n",
    "Please **implement Kaiming initialization** in the following codes. **You can only use NumPy**, although our codes will convert your results into PyTorch tensors later."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Kaiming for normal distribution\n",
    "def init_kaiming_normal(w):\n",
    "    assert (type(w) == np.ndarray), TypeError\n",
    "    assert len(w.shape) == 2, ValueError(\"W should be a matrix\")\n",
    "    result = w\n",
    "    ### TODO: your codes here\n",
    "    ### start i-1\n",
    "\n",
    "\n",
    "    ### end i-1\n",
    "    return(result)\n",
    "\n",
    "\n",
    "#  Kaiming for normal distribution\n",
    "def init_kaiming_uniform(w):\n",
    "    assert (type(w) == np.ndarray), TypeError\n",
    "    assert len(w.shape) == 2, ValueError(\"W should be a matrix\")\n",
    "    result = w\n",
    "    ### TODO: your codes here\n",
    "    ### start i-2\n",
    "\n",
    "    ### end i-2\n",
    "    return(result)"
   ]
  },
  {
   "source": [
    "## (j) Put them all together"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "epochs = 50\n",
    "log_interval = epochs\n",
    "optimizer = \"sgd\"\n",
    "\n",
    "### config\n",
    "\n",
    "model = create_mlp(L=4, N=16, n_0=2, n_L=1, activation=\"tanh\")\n",
    "for m in model.modules():\n",
    "    init_weights(m, method=\"xavier_normal\")\n",
    "\n",
    "model, loss_train_his = training(model, X_train_aug, y_train, lr=0.001, optimizer=optimizer, epochs=epochs, log_interval=log_interval)\n",
    "\n",
    "plt.plot(np.arange(epochs), loss_train_his, label=\"tanh, xavier_normal\")\n",
    "\n",
    "\n",
    "### config\n",
    "\n",
    "model = create_mlp(L=4, N=16, n_0=2, n_L=1, activation=\"tanh\")\n",
    "for m in model.modules():\n",
    "    init_weights(m, method=\"xavier_uniform\")\n",
    "\n",
    "model, loss_train_his = training(model, X_train_aug, y_train, lr=0.001, optimizer=optimizer, epochs=epochs, log_interval=log_interval)\n",
    "\n",
    "plt.plot(np.arange(epochs), loss_train_his, label=\"tanh, xavier_uniforml\")\n",
    "\n",
    "\n",
    "### config\n",
    "\n",
    "model = create_mlp(L=4, N=16, n_0=2, n_L=1, activation=\"tanh\")\n",
    "for m in model.modules():\n",
    "    init_weights(m, method=\"kaiming_normal\")\n",
    "\n",
    "model, loss_train_his = training(model, X_train_aug, y_train, lr=0.001, optimizer=optimizer, epochs=epochs, log_interval=log_interval)\n",
    "\n",
    "plt.plot(np.arange(epochs), loss_train_his, label=\"tanh, kaiming_normal\")\n",
    "\n",
    "\n",
    "### config\n",
    "\n",
    "model = create_mlp(L=4, N=16, n_0=2, n_L=1, activation=\"tanh\")\n",
    "for m in model.modules():\n",
    "    init_weights(m, method=\"kaiming_uniform\")\n",
    "\n",
    "model, loss_train_his = training(model, X_train_aug, y_train, lr=0.001, optimizer=optimizer, epochs=epochs, log_interval=log_interval)\n",
    "\n",
    "plt.plot(np.arange(epochs), loss_train_his, label=\"tanh, kaiming_uniform\")\n",
    "\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss (log-scale)\")\n",
    "plt.yscale(\"log\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "source": [
    "The above codes will train the same neural network with tanh activation using different initialization methods. **Run the above codes and write your obeservation** here."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### start j-1\n",
    "\n",
    "\n",
    "### end j-1\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "epochs = 50\n",
    "log_interval = epochs\n",
    "optimizer = \"sgd\"\n",
    "\n",
    "### config\n",
    "\n",
    "model = create_mlp(L=4, N=32, n_0=2, n_L=1, activation=\"relu\")\n",
    "for m in model.modules():\n",
    "    init_weights(m, method=\"xavier_normal\")\n",
    "\n",
    "model, loss_train_his = training(model, X_train_aug, y_train, lr=0.001, optimizer=optimizer, epochs=epochs, log_interval=log_interval)\n",
    "\n",
    "plt.plot(np.arange(epochs), loss_train_his, label=\"relu, xavier_normal\")\n",
    "\n",
    "\n",
    "### config\n",
    "\n",
    "model = create_mlp(L=4, N=32, n_0=2, n_L=1, activation=\"relu\")\n",
    "for m in model.modules():\n",
    "    init_weights(m, method=\"xavier_uniform\")\n",
    "\n",
    "model, loss_train_his = training(model, X_train_aug, y_train, lr=0.001, optimizer=optimizer, epochs=epochs, log_interval=log_interval)\n",
    "\n",
    "plt.plot(np.arange(epochs), loss_train_his, label=\"relu, xavier_uniforml\")\n",
    "\n",
    "\n",
    "### config\n",
    "\n",
    "model = create_mlp(L=4, N=32, n_0=2, n_L=1, activation=\"relu\")\n",
    "for m in model.modules():\n",
    "    init_weights(m, method=\"kaiming_normal\")\n",
    "\n",
    "model, loss_train_his = training(model, X_train_aug, y_train, lr=0.001, optimizer=optimizer, epochs=epochs, log_interval=log_interval)\n",
    "\n",
    "plt.plot(np.arange(epochs), loss_train_his, label=\"relu, kaiming_normal\")\n",
    "\n",
    "\n",
    "### config\n",
    "\n",
    "model = create_mlp(L=4, N=32, n_0=2, n_L=1, activation=\"relu\")\n",
    "for m in model.modules():\n",
    "    init_weights(m, method=\"kaiming_uniform\")\n",
    "\n",
    "model, loss_train_his = training(model, X_train_aug, y_train, lr=0.001, optimizer=optimizer, epochs=epochs, log_interval=log_interval)\n",
    "\n",
    "plt.plot(np.arange(epochs), loss_train_his, label=\"relu, kaiming_uniform\")\n",
    "\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss (log-scale)\")\n",
    "plt.yscale(\"log\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "source": [
    "The above codes will train the same neural network with tanh activation using different initialization methods. **Run the above codes and write your obeservation** here."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### start j-2\n",
    "\n",
    "<font color='blue'>\n",
    "\n",
    "\n",
    "\n",
    "</font>\n",
    "\n",
    "### end j-2"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}